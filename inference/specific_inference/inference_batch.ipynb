{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pressprexx/miniconda3/envs/unsloth/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from IPython.display import display\n",
    "from inference_utils import extract_level_representation, fix_level_format, fix_level_format_extra\n",
    "from create_img import convert_kidicarus_to_png, convert_loderunner_to_png, convert_mario_to_png, convert_rainbowisland_to_png\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "import io\n",
    "from PIL import Image\n",
    "from unsloth import FastModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import json\n",
    "from inference.metrics import SampledLevelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"llama-3\": [\n",
    "        \"/home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Llama-3.1-8B-Instruct-unsloth-bnb-4bit-mario-teste1\"\n",
    "    ],\n",
    "    \"qwen-3\": [\n",
    "        \"/home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Qwen3-14B-Instruct-bnb-4bit-mario-horizontal-newline-teste1\"\n",
    "    ],\n",
    "    \"qwen-2.5\": [\n",
    "        \"/home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Qwen-2.5-14b-horizontal-newline-1epoch-mario-teste1\"\n",
    "    ],\n",
    "    \"gemma-3\": [\n",
    "        \"/home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/gemma-3-12b-it-unsloth-bnb-4bit-mariogpt-teste1\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "temperatures = [0.7, 1.0, 1.2, 1.5, 2.0]\n",
    "\n",
    "num_of_samples = 5\n",
    "\n",
    "game_type = \"mario\"  # options: \"mario\", \"loderunner\", \"kidicarus\", \"rainbowisland\"\n",
    "orientation = \"horizontal\" #options: \"horizontal\", \"vertical\"\n",
    "\n",
    "# Game-specific settings\n",
    "game_settings = {\n",
    "    \"mario\": {\n",
    "        \"empty_space\": \"-\",\n",
    "        \"line_quantity\": 14,\n",
    "        \"column_quantity\": 50,\n",
    "        \"convert_function\": convert_mario_to_png,\n",
    "        \"tiles_dir\": '../../assets/mario',\n",
    "        \"add_ground\": \"X\",\n",
    "        \"expected_output_size\": 700\n",
    "    },\n",
    "    \"loderunner\": {\n",
    "        \"empty_space\": \".\",\n",
    "        \"line_quantity\": 22,\n",
    "        \"column_quantity\": 32,\n",
    "        \"convert_function\": convert_loderunner_to_png,\n",
    "        \"tiles_dir\": '../../assets/lode_runner',\n",
    "        \"add_ground\": None,\n",
    "        \"expected_output_size\": 704\n",
    "    },\n",
    "    \"kidicarus\": {\n",
    "        \"empty_space\": \"-\",\n",
    "        \"line_quantity\": 20,\n",
    "        \"column_quantity\": 16,\n",
    "        \"convert_function\": convert_kidicarus_to_png,\n",
    "        \"tiles_dir\": '../../assets/kid_icarus',\n",
    "        \"add_ground\": None,\n",
    "        \"expected_output_size\": 320\n",
    "    },\n",
    "    \"rainbowisland\": {\n",
    "        \"empty_space\": \".\",\n",
    "        \"line_quantity\": 35,\n",
    "        \"column_quantity\": 32,\n",
    "        \"convert_function\": convert_rainbowisland_to_png,\n",
    "        \"tiles_dir\": '../../assets/rainbow_island',\n",
    "        \"add_ground\": None,\n",
    "        \"expected_output_size\": 1120\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "prompt = \"Create a level\"\n",
    "output_pdf = f\"level_generation_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
    "max_seq_length = 2048\n",
    "dtype = None \n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_by_type(model_path, model_type, max_seq_length=2048, dtype=None, load_in_4bit=True):\n",
    "    \"\"\"Load model based on model type\"\"\"\n",
    "    if model_type in [\"llama-3\", \"qwen-2.5\"]:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    elif model_type == \"qwen-3\":\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    elif model_type == \"gemma-3\":\n",
    "        model, tokenizer = FastModel.from_pretrained(\n",
    "            model_name=model_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "            load_in_8bit=False,\n",
    "            full_finetuning=False,\n",
    "        )\n",
    "        FastModel.for_inference(model)\n",
    "        tokenizer = get_chat_template(\n",
    "            tokenizer,\n",
    "            chat_template=\"gemma-3\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_with_model(model, tokenizer, prompt, model_type, temperature=0.7, top_p=0.8, top_k=20):\n",
    "    \"\"\"Generate text with model based on model type\"\"\"\n",
    "    if model_type == \"gemma-3\":\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt,\n",
    "            }]\n",
    "        }]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        outputs = model.generate(\n",
    "            **tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=1024,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "    elif model_type == \"qwen-3\":\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False,\n",
    "        )\n",
    "        outputs = model.generate(\n",
    "            **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=512,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "    else:  # llama-3 or qwen-2.5\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=4096,\n",
    "            use_cache=True,\n",
    "            temperature=temperature,\n",
    "            min_p=0.1,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output = f\"level_generation_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "results_data = []\n",
    "evaluator = SampledLevelEvaluator()\n",
    "\n",
    "with PdfPages(output_pdf) as pdf:\n",
    "    for model_type, model_paths in models.items():\n",
    "        print(f\"Processing model type: {model_type}\")\n",
    "        \n",
    "        for model_path in model_paths:\n",
    "            print(f\"Processing model: {model_path}\")\n",
    "            \n",
    "            try:\n",
    "                model, tokenizer = load_model_by_type(\n",
    "                    model_path=model_path,\n",
    "                    model_type=model_type,\n",
    "                    max_seq_length=max_seq_length,\n",
    "                    dtype=dtype,\n",
    "                    load_in_4bit=load_in_4bit\n",
    "                )\n",
    "                \n",
    "                for temp in temperatures:\n",
    "                    print(f\"Running with temperature: {temp}\")\n",
    "                    \n",
    "                    for sample_idx in range(num_of_samples):\n",
    "                        print(f\"Generating sample {sample_idx+1}/{num_of_samples}\")\n",
    "                        \n",
    "                        generation_params = {\n",
    "                            'temperature': temp,\n",
    "                            'top_p': 0.8,\n",
    "                            'top_k': 20\n",
    "                        }\n",
    "                        \n",
    "                        if model_type == \"gemma-3\":\n",
    "                            generation_params['top_p'] = 0.95\n",
    "                            generation_params['top_k'] = 64\n",
    "                        \n",
    "                        response = generate_with_model(\n",
    "                            model=model,\n",
    "                            tokenizer=tokenizer,\n",
    "                            prompt=prompt,\n",
    "                            model_type=model_type,\n",
    "                            **generation_params\n",
    "                        )\n",
    "                        \n",
    "                        level = extract_level_representation(\n",
    "                            response[0], \n",
    "                            model_type=model_type, \n",
    "                            orientation=orientation, \n",
    "                        )\n",
    "                        \n",
    "                        fixed_level = fix_level_format_extra(\n",
    "                            level, \n",
    "                            empty_space=game_settings[game_type][\"empty_space\"], \n",
    "                            line_quantity=game_settings[game_type][\"line_quantity\"], \n",
    "                            column_quantity=game_settings[game_type][\"column_quantity\"], \n",
    "                            enforce_shape=\"both\", \n",
    "                            orientation=\"horizontal\",\n",
    "                            add_ground=game_settings[game_type][\"add_ground\"]\n",
    "                        )\n",
    "\n",
    "                        fixed_level = fixed_level.replace(\"|\", \"\\n\")\n",
    "                        \n",
    "                        expected_output_size = game_settings[game_type][\"expected_output_size\"]\n",
    "                        level_without_separators = level.replace(\"\\n\", \"\").replace(\"|\", \"\")\n",
    "                        diff_percentage = SampledLevelEvaluator.calculate_generation_diff(\n",
    "                            expected_output_size, \n",
    "                            level_without_separators\n",
    "                        )\n",
    "                        \n",
    "                        result_data = {\n",
    "                            \"model_type\": model_type,\n",
    "                            \"model_path\": os.path.basename(model_path),\n",
    "                            \"temperature\": temp,\n",
    "                            \"sample_index\": sample_idx + 1,\n",
    "                            \"level\": fixed_level,\n",
    "                            \"metrics\": {\n",
    "                                \"expected_size\": expected_output_size,\n",
    "                                \"actual_size\": len(level_without_separators),\n",
    "                                \"size_diff_percentage\": diff_percentage\n",
    "                            }\n",
    "                        }\n",
    "                        results_data.append(result_data)\n",
    "                        \n",
    "                        convert_function = game_settings[game_type][\"convert_function\"]\n",
    "                        tiles_dir = game_settings[game_type][\"tiles_dir\"]\n",
    "                        if tiles_dir:\n",
    "                            img, _, _ = convert_function(fixed_level, tiles_dir=tiles_dir)\n",
    "                        else:\n",
    "                            img, _, _ = convert_function(fixed_level)\n",
    "                        \n",
    "                        plt.figure(figsize=(12, 10))\n",
    "                        \n",
    "                        metadata = (\n",
    "                            f\"Model Type: {model_type}\\n\"\n",
    "                            f\"Model: {os.path.basename(model_path)}\\n\"\n",
    "                            f\"Temperature: {temp}\\n\"\n",
    "                            f\"Sample: {sample_idx+1}/{num_of_samples}\\n\"\n",
    "                            f\"Size Diff: {diff_percentage:.2f}%\\n\"\n",
    "                            f\"Level:\\n{fixed_level}\"\n",
    "                        )\n",
    "                        \n",
    "                        plt.subplot(2, 1, 1)\n",
    "                        plt.text(0.05, 0.95, metadata, fontsize=8, va='top', \n",
    "                                 family='monospace', transform=plt.gca().transAxes)\n",
    "                        plt.axis('off')\n",
    "                        plt.subplot(2, 1, 2)\n",
    "                        plt.imshow(img)\n",
    "                        plt.axis('off')\n",
    "                        plt.title(f\"Generated Level\")\n",
    "                        plt.tight_layout()\n",
    "\n",
    "                        pdf.savefig()\n",
    "                        plt.close()\n",
    "                        \n",
    "                        print(f\"Sample {sample_idx+1} completed\")\n",
    "                    \n",
    "                    print(f\"Completed temperature {temp}\")\n",
    "                \n",
    "                print(f\"Completed model {model_path}\")\n",
    "                \n",
    "                del model\n",
    "                del tokenizer\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                if model:\n",
    "                    del model\n",
    "                if tokenizer:\n",
    "                    del tokenizer\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                plt.figure(figsize=(8.5, 11))\n",
    "                error_info = (\n",
    "                    f\"Error processing model: {model_path}\\n\"\n",
    "                    f\"Model type: {model_type}\\n\\n\"\n",
    "                    f\"Error: {str(e)}\"\n",
    "                )\n",
    "                plt.text(0.5, 0.5, error_info, fontsize=12, ha='center', va='center', color='red')\n",
    "                plt.axis('off')\n",
    "                pdf.savefig()\n",
    "                plt.close()\n",
    "                print(f\"Error with model {model_path}: {str(e)}\")\n",
    "    \n",
    "    plt.figure(figsize=(8.5, 11))\n",
    "    \n",
    "    total_models = sum(len(paths) for paths in models.values())\n",
    "    \n",
    "    info = (\n",
    "        f\"Level Generation Results\\n\\n\"\n",
    "        f\"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        f\"Model Types: {', '.join(models.keys())}\\n\"\n",
    "        f\"Total Models: {total_models}\\n\"\n",
    "        f\"Temperatures: {temperatures}\\n\"\n",
    "        f\"Samples per combination: {num_of_samples}\\n\"\n",
    "        f\"Game type: {game_type}\\n\"\n",
    "        f\"Total samples: {total_models * len(temperatures) * num_of_samples}\"\n",
    "    )\n",
    "    plt.text(0.5, 0.5, info, fontsize=12, ha='center', va='center')\n",
    "    plt.axis('off')\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "# Save the JSON data\n",
    "with open(json_output, 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"PDF saved to {output_pdf}\")\n",
    "print(f\"JSON saved to {json_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
