{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pressprexx/miniconda3/envs/unsloth/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from IPython.display import display\n",
    "from inference_utils import extract_level_representation, fix_level_format, fix_level_format_extra\n",
    "from create_img import convert_kidicarus_to_png, convert_loderunner_to_png, convert_mario_to_png, convert_rainbowisland_to_png\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "import io\n",
    "from PIL import Image\n",
    "from unsloth import FastModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import json\n",
    "from inference.metrics import SampledLevelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"llama-3\": [\n",
    "        \"/home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Llama-3.1-8B-Instruct-unsloth-bnb-4bit-mario-teste1\"\n",
    "    ],\n",
    "    \"qwen-3\": [\n",
    "        \"/home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Qwen3-14B-Instruct-bnb-4bit-mario-horizontal-newline-teste1\"\n",
    "    ],\n",
    "    \"qwen-2.5\": [\n",
    "        \"/home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Qwen-2.5-14b-horizontal-newline-1epoch-mario-teste1\"\n",
    "    ],\n",
    "    \"gemma-3\": [\n",
    "        \"/home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/gemma-3-12b-it-unsloth-bnb-4bit-mariogpt-teste1\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "temperatures = [0.7, 1.0, 1.2, 1.5]\n",
    "\n",
    "num_of_samples = 3\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "game_type = \"mario\"  # options: \"mario\", \"loderunner\", \"kidicarus\", \"rainbowisland\"\n",
    "\n",
    "# Game-specific settings\n",
    "game_settings = {\n",
    "    \"mario\": {\n",
    "        \"empty_space\": \"-\",\n",
    "        \"line_quantity\": 14,\n",
    "        \"column_quantity\": 50,\n",
    "        \"convert_function\": convert_mario_to_png,\n",
    "        \"tiles_dir\": '../../assets/mario',\n",
    "        \"add_ground\": \"X\",\n",
    "        \"expected_output_size\": 700\n",
    "    },\n",
    "    \"loderunner\": {\n",
    "        \"empty_space\": \".\",\n",
    "        \"line_quantity\": 22,\n",
    "        \"column_quantity\": 32,\n",
    "        \"convert_function\": convert_loderunner_to_png,\n",
    "        \"tiles_dir\": '../../assets/lode_runner',\n",
    "        \"add_ground\": None,\n",
    "        \"expected_output_size\": 704\n",
    "    },\n",
    "    \"kidicarus\": {\n",
    "        \"empty_space\": \"-\",\n",
    "        \"line_quantity\": 20,\n",
    "        \"column_quantity\": 16,\n",
    "        \"convert_function\": convert_kidicarus_to_png,\n",
    "        \"tiles_dir\": '../../assets/kid_icarus',\n",
    "        \"add_ground\": None,\n",
    "        \"expected_output_size\": 320\n",
    "    },\n",
    "    \"rainbowisland\": {\n",
    "        \"empty_space\": \".\",\n",
    "        \"line_quantity\": 35,\n",
    "        \"column_quantity\": 32,\n",
    "        \"convert_function\": convert_rainbowisland_to_png,\n",
    "        \"tiles_dir\": '../../assets/rainbow_island',\n",
    "        \"add_ground\": None,\n",
    "        \"expected_output_size\": 1120\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "prompt = \"Create a level\"\n",
    "output_pdf = f\"level_generation_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_by_type(model_path, model_type, max_seq_length=2048, dtype=None, load_in_4bit=True):\n",
    "    \"\"\"Load model based on model type\"\"\"\n",
    "    if model_type in [\"llama-3\", \"qwen-2.5\"]:\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    elif model_type == \"qwen-3\":\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    elif model_type == \"gemma-3\":\n",
    "        model, tokenizer = FastModel.from_pretrained(\n",
    "            model_name=model_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "            load_in_8bit=False,\n",
    "            full_finetuning=False,\n",
    "        )\n",
    "        FastModel.for_inference(model)\n",
    "        tokenizer = get_chat_template(\n",
    "            tokenizer,\n",
    "            chat_template=\"gemma-3\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_with_model(model, tokenizer, prompt, model_type, temperature=0.7, top_p=0.8, top_k=20):\n",
    "    \"\"\"Generate text with model based on model type\"\"\"\n",
    "    if model_type == \"gemma-3\":\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt,\n",
    "            }]\n",
    "        }]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        outputs = model.generate(\n",
    "            **tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=1024,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "    elif model_type == \"qwen-3\":\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False,\n",
    "        )\n",
    "        outputs = model.generate(\n",
    "            **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=512,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "    else:  # llama-3 or qwen-2.5\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=4096,\n",
    "            use_cache=True,\n",
    "            temperature=temperature,\n",
    "            min_p=0.1,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model type: llama-3\n",
      "Processing model: /home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Llama-3.1-8B-Instruct-unsloth-bnb-4bit-mario-teste1\n",
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.677 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with temperature: 0.7\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 0.7\n",
      "Running with temperature: 1.0\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.0\n",
      "Running with temperature: 1.2\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.2\n",
      "Running with temperature: 1.5\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.5\n",
      "Completed model /home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Llama-3.1-8B-Instruct-unsloth-bnb-4bit-mario-teste1\n",
      "Processing model type: qwen-3\n",
      "Processing model: /home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Qwen3-14B-Instruct-bnb-4bit-mario-horizontal-newline-teste1\n",
      "==((====))==  Unsloth 2025.4.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.677 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.26it/s]\n",
      "Unsloth 2025.4.7 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with temperature: 0.7\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 0.7\n",
      "Running with temperature: 1.0\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.0\n",
      "Running with temperature: 1.2\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.2\n",
      "Running with temperature: 1.5\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.5\n",
      "Completed model /home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Qwen3-14B-Instruct-bnb-4bit-mario-horizontal-newline-teste1\n",
      "Processing model type: qwen-2.5\n",
      "Processing model: /home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Qwen-2.5-14b-horizontal-newline-1epoch-mario-teste1\n",
      "==((====))==  Unsloth 2025.4.7: Fast Qwen2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.677 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.48it/s]\n",
      "Unsloth 2025.4.7 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with temperature: 0.7\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "REPLACING <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=16x16 at 0x70340540B790> (24, 12)\n",
      "REPLACING <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=16x16 at 0x70340540B790> (25, 12)\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 0.7\n",
      "Running with temperature: 1.0\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.0\n",
      "Running with temperature: 1.2\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.2\n",
      "Running with temperature: 1.5\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.5\n",
      "Completed model /home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/Qwen-2.5-14b-horizontal-newline-1epoch-mario-teste1\n",
      "Processing model type: gemma-3\n",
      "Processing model: /home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/gemma-3-12b-it-unsloth-bnb-4bit-mariogpt-teste1\n",
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.677 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.76it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with temperature: 0.7\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 0.7\n",
      "Running with temperature: 1.0\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.0\n",
      "Running with temperature: 1.2\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.2\n",
      "Running with temperature: 1.5\n",
      "Generating sample 1/3\n",
      "Sample 1 completed\n",
      "Generating sample 2/3\n",
      "Sample 2 completed\n",
      "Generating sample 3/3\n",
      "Sample 3 completed\n",
      "Completed temperature 1.5\n",
      "Completed model /home/pressprexx/Code/AKCITGaming/Paper_LLM_PCG_Geral/VGLC_LLM_Finetunning/models/mario/gemma-3-12b-it-unsloth-bnb-4bit-mariogpt-teste1\n",
      "PDF saved to level_generation_results_20250521_100847.pdf\n",
      "JSON saved to level_generation_results_20250521_100847.json\n"
     ]
    }
   ],
   "source": [
    "json_output = f\"level_generation_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "results_data = []\n",
    "evaluator = SampledLevelEvaluator()\n",
    "\n",
    "with PdfPages(output_pdf) as pdf:\n",
    "    for model_type, model_paths in models.items():\n",
    "        print(f\"Processing model type: {model_type}\")\n",
    "        \n",
    "        for model_path in model_paths:\n",
    "            print(f\"Processing model: {model_path}\")\n",
    "            \n",
    "            try:\n",
    "                model, tokenizer = load_model_by_type(\n",
    "                    model_path=model_path,\n",
    "                    model_type=model_type,\n",
    "                    max_seq_length=max_seq_length,\n",
    "                    dtype=dtype,\n",
    "                    load_in_4bit=load_in_4bit\n",
    "                )\n",
    "                \n",
    "                for temp in temperatures:\n",
    "                    print(f\"Running with temperature: {temp}\")\n",
    "                    \n",
    "                    for sample_idx in range(num_of_samples):\n",
    "                        print(f\"Generating sample {sample_idx+1}/{num_of_samples}\")\n",
    "                        \n",
    "                        generation_params = {\n",
    "                            'temperature': temp,\n",
    "                            'top_p': 0.8,\n",
    "                            'top_k': 20\n",
    "                        }\n",
    "                        \n",
    "                        if model_type == \"gemma-3\":\n",
    "                            generation_params['top_p'] = 0.95\n",
    "                            generation_params['top_k'] = 64\n",
    "                        \n",
    "                        response = generate_with_model(\n",
    "                            model=model,\n",
    "                            tokenizer=tokenizer,\n",
    "                            prompt=prompt,\n",
    "                            model_type=model_type,\n",
    "                            **generation_params\n",
    "                        )\n",
    "                        \n",
    "                        level = extract_level_representation(\n",
    "                            response[0], \n",
    "                            model_type=model_type, \n",
    "                            orientation=\"horizontal\", \n",
    "                            separator=\"\\n\"\n",
    "                        )\n",
    "                        \n",
    "                        fixed_level = fix_level_format_extra(\n",
    "                            level, \n",
    "                            empty_space=game_settings[game_type][\"empty_space\"], \n",
    "                            line_quantity=game_settings[game_type][\"line_quantity\"], \n",
    "                            column_quantity=game_settings[game_type][\"column_quantity\"], \n",
    "                            enforce_shape=\"both\", \n",
    "                            add_ground=game_settings[game_type][\"add_ground\"]\n",
    "                        )\n",
    "                        \n",
    "                        expected_output_size = game_settings[game_type][\"expected_output_size\"]\n",
    "                        level_without_separators = level.replace(\"\\n\", \"\").replace(\"|\", \"\")\n",
    "                        diff_percentage = SampledLevelEvaluator.calculate_generation_diff(\n",
    "                            expected_output_size, \n",
    "                            level_without_separators\n",
    "                        )\n",
    "                        \n",
    "                        result_data = {\n",
    "                            \"model_type\": model_type,\n",
    "                            \"model_path\": os.path.basename(model_path),\n",
    "                            \"temperature\": temp,\n",
    "                            \"sample_index\": sample_idx + 1,\n",
    "                            \"level\": fixed_level,\n",
    "                            \"metrics\": {\n",
    "                                \"expected_size\": expected_output_size,\n",
    "                                \"actual_size\": len(level_without_separators),\n",
    "                                \"size_diff_percentage\": diff_percentage\n",
    "                            }\n",
    "                        }\n",
    "                        results_data.append(result_data)\n",
    "                        \n",
    "                        convert_function = game_settings[game_type][\"convert_function\"]\n",
    "                        tiles_dir = game_settings[game_type][\"tiles_dir\"]\n",
    "                        if tiles_dir:\n",
    "                            img, _, _ = convert_function(fixed_level, tiles_dir=tiles_dir)\n",
    "                        else:\n",
    "                            img, _, _ = convert_function(fixed_level)\n",
    "                        \n",
    "                        plt.figure(figsize=(12, 10))\n",
    "                        \n",
    "                        metadata = (\n",
    "                            f\"Model Type: {model_type}\\n\"\n",
    "                            f\"Model: {os.path.basename(model_path)}\\n\"\n",
    "                            f\"Temperature: {temp}\\n\"\n",
    "                            f\"Sample: {sample_idx+1}/{num_of_samples}\\n\"\n",
    "                            f\"Size Diff: {diff_percentage:.2f}%\\n\"\n",
    "                            f\"Level:\\n{fixed_level}\"\n",
    "                        )\n",
    "                        \n",
    "                        plt.subplot(2, 1, 1)\n",
    "                        plt.text(0.05, 0.95, metadata, fontsize=8, va='top', \n",
    "                                 family='monospace', transform=plt.gca().transAxes)\n",
    "                        plt.axis('off')\n",
    "                        plt.subplot(2, 1, 2)\n",
    "                        plt.imshow(img)\n",
    "                        plt.axis('off')\n",
    "                        plt.title(f\"Generated Level\")\n",
    "                        plt.tight_layout()\n",
    "\n",
    "                        pdf.savefig()\n",
    "                        plt.close()\n",
    "                        \n",
    "                        print(f\"Sample {sample_idx+1} completed\")\n",
    "                    \n",
    "                    print(f\"Completed temperature {temp}\")\n",
    "                \n",
    "                print(f\"Completed model {model_path}\")\n",
    "                \n",
    "                del model\n",
    "                del tokenizer\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                if model:\n",
    "                    del model\n",
    "                if tokenizer:\n",
    "                    del tokenizer\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                plt.figure(figsize=(8.5, 11))\n",
    "                error_info = (\n",
    "                    f\"Error processing model: {model_path}\\n\"\n",
    "                    f\"Model type: {model_type}\\n\\n\"\n",
    "                    f\"Error: {str(e)}\"\n",
    "                )\n",
    "                plt.text(0.5, 0.5, error_info, fontsize=12, ha='center', va='center', color='red')\n",
    "                plt.axis('off')\n",
    "                pdf.savefig()\n",
    "                plt.close()\n",
    "                print(f\"Error with model {model_path}: {str(e)}\")\n",
    "    \n",
    "    plt.figure(figsize=(8.5, 11))\n",
    "    \n",
    "    total_models = sum(len(paths) for paths in models.values())\n",
    "    \n",
    "    info = (\n",
    "        f\"Level Generation Results\\n\\n\"\n",
    "        f\"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        f\"Model Types: {', '.join(models.keys())}\\n\"\n",
    "        f\"Total Models: {total_models}\\n\"\n",
    "        f\"Temperatures: {temperatures}\\n\"\n",
    "        f\"Samples per combination: {num_of_samples}\\n\"\n",
    "        f\"Game type: {game_type}\\n\"\n",
    "        f\"Total samples: {total_models * len(temperatures) * num_of_samples}\"\n",
    "    )\n",
    "    plt.text(0.5, 0.5, info, fontsize=12, ha='center', va='center')\n",
    "    plt.axis('off')\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "# Save the JSON data\n",
    "with open(json_output, 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"PDF saved to {output_pdf}\")\n",
    "print(f\"JSON saved to {json_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
